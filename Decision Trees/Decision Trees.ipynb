{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are versatile Machine Learning algorithms that can perform both  classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets. \n",
    "\n",
    "**Limitations:**\n",
    "- Orthogonal Decison Boundaries.\n",
    "- Sensitive to Variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pimg\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor,export_graphviz\n",
    "from sklearn.datasets import load_boston, load_wine\n",
    "\n",
    "from IPython.display import display, Math, Latex, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Understanding Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Recognition Dataset\n",
    "\n",
    "**Description:**\n",
    "- Alcohol\n",
    "- Malic acid\n",
    "- Ash\n",
    "- Alcalinity of ash\n",
    "- Magnesium\n",
    "- Total phenols\n",
    "- Flavanoids\n",
    "- Nonflavanoid phenols\n",
    "- Proanthocyanins\n",
    "- Color intensity\n",
    "- Hue\n",
    "- OD280/OD315 of diluted wines\n",
    "- Proline\n",
    "\n",
    "This is a Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  Class  \n",
       "0                            3.92   1065.0    0.0  \n",
       "1                            3.40   1050.0    0.0  \n",
       "2                            3.17   1185.0    0.0  \n",
       "3                            3.45   1480.0    0.0  \n",
       "4                            2.93    735.0    0.0  \n",
       "..                            ...      ...    ...  \n",
       "173                          1.74    740.0    2.0  \n",
       "174                          1.56    750.0    2.0  \n",
       "175                          1.56    835.0    2.0  \n",
       "176                          1.62    840.0    2.0  \n",
       "177                          1.60    560.0    2.0  \n",
       "\n",
       "[178 rows x 14 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95.0</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102.0</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96.0</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "ClassificationDataset = load_wine()\n",
    "Xc = ClassificationDataset['data']\n",
    "yc = ClassificationDataset['target']\n",
    "Featuresc = ClassificationDataset['feature_names']\n",
    "TargetsNames = ClassificationDataset['target_names']\n",
    "\n",
    "dfc = pd.DataFrame(data=np.append(Xc,np.expand_dims(yc,axis=-1),axis=1), columns=Featuresc + ['Class'])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston House Prices Dataset\n",
    "**Description:**\n",
    "- CRIM per capita crime rate by town\n",
    "- ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS proportion of non-retail business acres per town\n",
    "- CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX nitric oxides concentration (parts per 10 million)\n",
    "- RM average number of rooms per dwelling\n",
    "- AGE proportion of owner-occupied units built prior to 1940\n",
    "- DIS weighted distances to five Boston employment centres\n",
    "- RAD index of accessibility to radial highways\n",
    "- TAX full-value property-tax rate per $10,000$\n",
    "- PTRATIO pupil-teacher ratio by town\n",
    "- B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT % lower status of the population\n",
    "- MEDV Median value of owner-occupied homes in 1000’s\n",
    "\n",
    "This is a Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.06263</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.593</td>\n      <td>69.1</td>\n      <td>2.4786</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>391.99</td>\n      <td>9.67</td>\n      <td>22.4</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>0.04527</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.120</td>\n      <td>76.7</td>\n      <td>2.2875</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>9.08</td>\n      <td>20.6</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>0.06076</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.976</td>\n      <td>91.0</td>\n      <td>2.1675</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>5.64</td>\n      <td>23.9</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>0.10959</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.794</td>\n      <td>89.3</td>\n      <td>2.3889</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>393.45</td>\n      <td>6.48</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>0.04741</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0.0</td>\n      <td>0.573</td>\n      <td>6.030</td>\n      <td>80.8</td>\n      <td>2.5050</td>\n      <td>1.0</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>7.88</td>\n      <td>11.9</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "RegressionDataset = load_boston()\n",
    "Xr = RegressionDataset['data']\n",
    "yr = RegressionDataset['target']\n",
    "Featuresr = RegressionDataset['feature_names'].tolist()\n",
    "\n",
    "dfr = pd.DataFrame(data=np.append(Xr,np.expand_dims(yr,axis=-1),axis=1), columns= Featuresr + ['MEDV'])\n",
    "dfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "Training and Understanding Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "Classifier = DecisionTreeClassifier(max_depth=5)\n",
    "Classifier.fit(Xc,yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "        Classifier,\n",
    "        out_file = \"Images/WineRecognition.dot\",\n",
    "        feature_names=Featuresc,\n",
    "        class_names=TargetsNames,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "        )\n",
    "!dot -Tpng Images/WineRecognition.dot -o Images/WineRecognition.png"
   ]
  },
  {
   "source": [
    "### Understanding Decision Tree Classifier\n",
    "\n",
    "**Decision Tree of Wine Recognition**\n",
    "\n",
    "<img src=\"Images/WineRecognition.png\" style=\"width:1250px; height:750px\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** Description:**\n",
    "- Evaluates the condition, if **True** then goes to left child node else to the right child node.\n",
    "- If the leaf node does not have any children nodes, so it does not ask any questions and we can simply look at the predicted class for that node and the Decision Tree predicts.\n",
    "- One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don’t require feature scaling or centering at all.\n",
    "- A node’s samples attribute counts how many training instances it applies to.\n",
    "- A node’s gini attribute measures its impurity: a node is \"pure\" (gini=0) if all training instances it applies to belong to the same class.\n",
    "- Gini Coefficient $G_i$ \\begin{align} G_i = 1 - \\sum_{k=1}^{n} p_{i,k}^2 \\end{align}\n",
    "- $p_{i,k}$ is the ratio of class k instances among the training instances in the $i^{th}$ node.\n",
    "- Decision Trees are fairly intuitive and their decisions are easy to interpret. Such models are often called white box models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Estimating Class Probabilities\n",
    "A Decision Tree can also estimate the probability that an instance belongs to a particular class k"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "Classifier.predict_proba(np.random.randint(10)*np.random.rand(1,Xc.shape[1]))"
   ]
  },
  {
   "source": [
    "## Decision Tree Regressor\n",
    "Tranining and Understanding Decision Tree Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(min_samples_leaf=10)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(min_samples_leaf=10)\n",
    "Regressor.fit(Xr,yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "        Regressor,\n",
    "        out_file = \"Images/BostonHousePrices.dot\",\n",
    "        feature_names=Featuresc,\n",
    "        class_names=TargetsNames,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "        )\n",
    "!dot -Tpng Images/BostonHousePrices.dot -o Images/BostonHousePrices.png"
   ]
  },
  {
   "source": [
    "### Understanding Decision Tree Regressor\n",
    "\n",
    "**Decision Tree of Boston Houses Prices**\n",
    "\n",
    "<img src=\"Images/BostonHousePrices.png\">\n",
    "\n",
    "**Description:**\n",
    "- Predictions in case of Decision Tree Regressor is Average of Target Values of all Instances.\n",
    "- Decision Tress are prone to Overfit when dealing with Regression Tasks similar to Classification Tasks. So, inorder to avoid Overfitting of Decision Tree we perform Regularisation.\n",
    "\n",
    "**Regularisation:**\n",
    "Increasing **min_** hyperparameters or Decreasing **max_** hyperparameters regularize the model.\n",
    "- min_samples_split: Minimum number of samples a node must have before it can be split.\n",
    "- min_samples_leaf: Minimum number of samples a leaf node must have.\n",
    "- min_weight_fraction_leaf: A fraction of the total number of weighted instances a node must have. \n",
    "- max_depth: Depth of Decision Tree.\n",
    "- max_leaf_nodes: Maximum number of leaf nodes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}